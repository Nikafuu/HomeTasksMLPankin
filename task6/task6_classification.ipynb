{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-Mb7bnoaRLM"
   },
   "source": [
    "# Логичтическая регрессия, метод опорных векторов, one-hot кодирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNyXo3CvaRLP"
   },
   "source": [
    "### О задании\n",
    "\n",
    "В этом задании вы:\n",
    "- настроите метод опорных векторов\n",
    "- изучите методы работы с категориальными переменными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ndj090dOaRLQ",
    "outputId": "fdef6337-1efe-4945-d445-fb17c46dcc4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "import pandas as pd  # тут импорт для работы с данными\n",
    "from joblib import Parallel, delayed  # еще импорт для параллельного выполнения задач\n",
    "import warnings, time  # эти модули для предупреждений и времени\n",
    "warnings.simplefilter(action='ignore')  # игнорируем предупреждения\n",
    "from sklearn.base import BaseEstimator  # это типа базовый класс для оценщиков\n",
    "from sklearn.datasets import load_diabetes  # импорт функции для загрузки датасета\n",
    "from sklearn.model_selection import train_test_split  # еще импорт для деления данных\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # функции для оценки качества моделей\n",
    "import copy  # копирование объектов\n",
    "import pandas as pd  # тут снова импорт pandas\n",
    "from sklearn.model_selection import train_test_split  # опять импортируем функцию для деления данных\n",
    "from sklearn.linear_model import LogisticRegression  # для логистической регрессии\n",
    "from sklearn.preprocessing import OneHotEncoder  # кодирование категориальных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_wS0x7RaRLR"
   },
   "source": [
    "__Задание 1.__ Обучение логистической регрессии на реальных данных и оценка качества классификации.\n",
    "\n",
    "**(5 баллов)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94eHa5RgaRLS"
   },
   "source": [
    "Загрузим данные с конкурса [Kaggle Porto Seguro’s Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction) (вам нужна только обучающая выборка). Задача состоит в определении водителей, которые в ближайший год воспользуются своей автомобильной страховкой (бинарная классификация). Но для нас важна будет не сама задача, а только её данные. При этом под нужды задания мы немного модифицируем датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BJQn-94DaRLS"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', index_col=0)  # читаем файл 'train.csv' и делаем первый столбец индексом\n",
    "target = data.target.values  # сохраняем значения столбца 'target' как массив целевых переменных\n",
    "data = data.drop('target', axis=1)  # удаляем столбец 'target', axis=1 для удаления по столбцам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2Su7GNhaRLT"
   },
   "source": [
    "Пересемплируем выборку так, чтобы положительных и отрицательных объектов в выборке было одинаковое число. Разделим на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fot9A7L8aRLT"
   },
   "outputs": [],
   "source": [
    "np.random.seed(910)  # ну типа старт генерации случайных чисел\n",
    "mask_plus = np.random.choice(np.where(target == 1)[0], 100000, replace=True)  # выбор случайных значений, когда цель это 1\n",
    "mask_zero = np.random.choice(np.where(target == 0)[0], 100000, replace=True)  # выбор случайных значений, когда цель это 0\n",
    "mask = np.concatenate([mask_plus, mask_zero])  # ага, объединяем всё это в один массив\n",
    "mask = np.sort(mask)  # ну и отсортировали его, чтоб было аккуратно\n",
    "\n",
    "data = data.iloc[mask]  # применяем маску к данным, получается вот такой фильтр\n",
    "target = target[mask]  # а это уже фильтр для целевых значений\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.5, random_state=73)  # ну и делим всё это на обучающие и тестовые наборы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GB0kVSoaRLT"
   },
   "source": [
    "Не забудьте отнормировать признаки (можно воспользоваться StandardScaler или сделать это вручную). Пока не будем обращать внимание на то, что некоторые признаки категориальные (этим мы займёмся позже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5dDctZhDaRLU"
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    new_data = copy.deepcopy(data)  # делаем копию данных, чтобы не изменять оригинальные\n",
    "    for c in data.columns:  # проходим по всем столбцам в данных\n",
    "        new_data[c] = (new_data[c] - new_data[c].min()) / (new_data[c].max() - new_data[c].min())  # нормализуем значения столбца\n",
    "    return new_data  # возвращаем нормализованные данные\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSrjeimpaRLU"
   },
   "source": [
    "Обучите логистическую регрессию с удобными для вас параметрами, примените регуляризацию. Сделайте предсказание на тестовой части выборки. Посчитайте accuracy, precision, recall и F меру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "A1tEHyNFaRLU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5885\n",
      "Precision: 0.5971422338568936\n",
      "Recall: 0.5468475307655426\n",
      "F1 Score: 0.5708892967381329\n",
      "Time taken: 3.6531147956848145 s\n"
     ]
    }
   ],
   "source": [
    "def get(X_train, X_test, y_train, y_test):  # объявляем функцию get с параметрами X_train, X_test, y_train, y_test\n",
    "    st = time.time()  # запоминаем текущее время\n",
    "    model = LogisticRegression(max_iter=1000)  # создаем модель логистической регрессии с максимальным количеством итераций 1000\n",
    "    model.fit(X_train, y_train)  # обучаем модель на обучающих данных\n",
    "    y_pred = model.predict(X_test)  # делаем прогноз на тестовых данных\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # вычисляем точность прогноза\n",
    "    precision = precision_score(y_test, y_pred)  # вычисляем точность\n",
    "    recall = recall_score(y_test, y_pred)  # вычисляем полноту\n",
    "    f1 = f1_score(y_test, y_pred)  # вычисляем F1-меру\n",
    "    print(f\"Accuracy:\", accuracy)  # выводим точность\n",
    "    print(f\"Precision:\", precision)  # выводим точность\n",
    "    print(f\"Recall:\", recall)  # выводим полноту\n",
    "    print(f\"F1 Score:\", f1)  # выводим F1-меру\n",
    "    en = time.time()  # запоминаем текущее время\n",
    "    print(f\"Time taken: {en - st} s\")  # выводим время выполнения функции\n",
    "\n",
    "\n",
    "get(*train_test_split(normalize(data), target, test_size=0.5, random_state=73))  # вызываем функцию get с аргументами, полученными из train_test_split и normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9UQ31XFaRLU"
   },
   "source": [
    "__Выводы__ в свободной форме:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHvrjAQnaRLW"
   },
   "source": [
    "## Часть 2. Работа с категориальными переменными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_doKeEKxaRLW"
   },
   "source": [
    "В этой части мы научимся обрабатывать категориальные переменные, так как закодировать их в виде чисел недостаточно (это задаёт некоторый порядок, которого на категориальных переменных может и не быть). Существует два основных способа обработки категориальных значений:\n",
    "- One-hot-кодирование\n",
    "- Счётчики (CTR, mean-target кодирование, ...) — каждый категориальный признак заменяется на среднее значение целевой переменной по всем объектам, имеющим одинаковое значение в этом признаке.\n",
    "\n",
    "Начнём с one-hot-кодирования. Допустим наш категориальный признак $f_j(x)$ принимает значения из множества $C=\\{c_1, \\dots, c_m\\}$. Заменим его на $m$ бинарных признаков $b_1(x), \\dots, b_m(x)$, каждый из которых является индикатором одного из возможных категориальных значений:\n",
    "$$\n",
    "b_i(x) = [f_j(x) = c_i]\n",
    "$$\n",
    "\n",
    "__Задание 1.__ Закодируйте все категориальные признаки с помощью one-hot-кодирования. Обучите логистическую регрессию и посмотрите, как изменилось качество модели (с тем, что было ранее). Измерьте время, потребовавшееся на обучение модели.\n",
    "\n",
    "__(3 балла)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GC4tPzPbaRLW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.998215</td>\n",
       "      <td>1.366550</td>\n",
       "      <td>4.483870</td>\n",
       "      <td>0.429490</td>\n",
       "      <td>0.502265</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.295375</td>\n",
       "      <td>0.175920</td>\n",
       "      <td>0.177605</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>...</td>\n",
       "      <td>5.443865</td>\n",
       "      <td>1.443745</td>\n",
       "      <td>2.873590</td>\n",
       "      <td>7.544455</td>\n",
       "      <td>0.123355</td>\n",
       "      <td>0.630875</td>\n",
       "      <td>0.553405</td>\n",
       "      <td>0.287530</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.017199</td>\n",
       "      <td>0.674421</td>\n",
       "      <td>2.739255</td>\n",
       "      <td>0.496689</td>\n",
       "      <td>1.501934</td>\n",
       "      <td>0.477315</td>\n",
       "      <td>0.456212</td>\n",
       "      <td>0.380753</td>\n",
       "      <td>0.382181</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>...</td>\n",
       "      <td>2.342462</td>\n",
       "      <td>1.201163</td>\n",
       "      <td>1.692875</td>\n",
       "      <td>2.745287</td>\n",
       "      <td>0.328845</td>\n",
       "      <td>0.482569</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.452612</td>\n",
       "      <td>0.475369</td>\n",
       "      <td>0.359796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ps_ind_01  ps_ind_02_cat      ps_ind_03  ps_ind_04_cat  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.998215       1.366550       4.483870       0.429490   \n",
       "std         2.017199       0.674421       2.739255       0.496689   \n",
       "min         0.000000      -1.000000       0.000000      -1.000000   \n",
       "25%         0.000000       1.000000       2.000000       0.000000   \n",
       "50%         1.000000       1.000000       4.000000       0.000000   \n",
       "75%         3.000000       2.000000       7.000000       1.000000   \n",
       "max         7.000000       4.000000      11.000000       1.000000   \n",
       "\n",
       "       ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.502265       0.351100       0.295375       0.175920   \n",
       "std         1.501934       0.477315       0.456212       0.380753   \n",
       "min        -1.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       1.000000       1.000000       0.000000   \n",
       "max         6.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       ps_ind_09_bin  ps_ind_10_bin  ...     ps_calc_11     ps_calc_12  \\\n",
       "count  200000.000000  200000.000000  ...  200000.000000  200000.000000   \n",
       "mean        0.177605       0.000470  ...       5.443865       1.443745   \n",
       "std         0.382181       0.021674  ...       2.342462       1.201163   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       4.000000       1.000000   \n",
       "50%         0.000000       0.000000  ...       5.000000       1.000000   \n",
       "75%         0.000000       0.000000  ...       7.000000       2.000000   \n",
       "max         1.000000       1.000000  ...      18.000000       8.000000   \n",
       "\n",
       "          ps_calc_13     ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  \\\n",
       "count  200000.000000  200000.000000   200000.000000   200000.000000   \n",
       "mean        2.873590       7.544455        0.123355        0.630875   \n",
       "std         1.692875       2.745287        0.328845        0.482569   \n",
       "min         0.000000       0.000000        0.000000        0.000000   \n",
       "25%         2.000000       6.000000        0.000000        0.000000   \n",
       "50%         3.000000       7.000000        0.000000        1.000000   \n",
       "75%         4.000000       9.000000        0.000000        1.000000   \n",
       "max        13.000000      22.000000        1.000000        1.000000   \n",
       "\n",
       "       ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "count   200000.000000   200000.000000   200000.000000   200000.000000  \n",
       "mean         0.553405        0.287530        0.345000        0.152800  \n",
       "std          0.497141        0.452612        0.475369        0.359796  \n",
       "min          0.000000        0.000000        0.000000        0.000000  \n",
       "25%          0.000000        0.000000        0.000000        0.000000  \n",
       "50%          1.000000        0.000000        0.000000        0.000000  \n",
       "75%          1.000000        1.000000        1.000000        0.000000  \n",
       "max          1.000000        1.000000        1.000000        1.000000  \n",
       "\n",
       "[8 rows x 57 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()  # вот тут мы выводим статистическое описание данных, ну так, чтобы понять, что в них происходит"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izITXcOWaRLW"
   },
   "source": [
    "Как можно было заменить, one-hot-кодирование может сильно увеличивать количество признаков в датасете, что сказывается на памяти, особенно, если некоторый признак имеет большое количество значений. Эту проблему решает другой способ кодирование категориальных признаков — счётчики. Основная идея в том, что нам важны не сами категории, а значения целевой переменной, которые имеют объекты этой категории. Каждый категориальный признак мы заменим средним значением целевой переменной по всем объектам этой же категории:\n",
    "$$\n",
    "g_j(x, X) = \\frac{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)][y_i = +1]}{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)]}\n",
    "$$\n",
    "\n",
    "__Задание 2.__ Закодируйте категориальные переменные с помощью счётчиков (ровно так, как описано выше без каких-либо хитростей). Обучите логистическую регрессию и посмотрите на качество модели на тестовом множестве. Сравните время обучения с предыдущим экспериментов. Заметили ли вы что-то интересное?\n",
    "\n",
    "__(2 балла)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vJmhJjcyaRLW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59355\n",
      "Precision: 0.6014488066748593\n",
      "Recall: 0.5573158062969474\n",
      "F1 Score: 0.578541876211906\n",
      "Time taken: 24.93733859062195 s\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [c for c in data.columns if c.endswith('_cat')]  # выбираем категориальные признаки, которые заканчиваются на '_cat'\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=\"first\")  # создаем кодировщик OneHotEncoder с параметрами sparse_output=False и drop=\"first\"\n",
    "encoded_features = encoder.fit_transform(data[categorical_features])  # кодируем категориальные признаки\n",
    "\n",
    "data_one_hot = data.drop(categorical_features, axis=1)  # создаем датафрейм без категориальных признаков\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))  # создаем датафрейм с закодированными признаками\n",
    "\n",
    "data_one_hot.reset_index(inplace=True, drop=True)  # сбрасываем индексы\n",
    "data_one_hot = pd.concat([data_one_hot, encoded_df], axis=1)  # объединяем исходные данные с закодированными признаками\n",
    "\n",
    "get(*train_test_split(normalize(data_one_hot), target, test_size=0.5, random_state=73))  # запускаем функцию get с аргументами, полученными из train_test_split и normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ6BybtVaRLW"
   },
   "source": [
    "__Вывод:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUSCGlbhaRLW"
   },
   "source": [
    "Отметим, что такие признаки сами по себе являются классификаторами и, обучаясь на них, мы допускаем \"утечку\" целевой переменной в признаки. Это ведёт к переобучению, поэтому считать такие признаки необходимо таким образом, чтобы при вычислении для конкретного объекта его целевая метка не использовалась. Это можно делать следующими способами:\n",
    "- вычислять значение счётчика по всем объектам расположенным выше в датасете (например, если у нас выборка отсортирована по времени)\n",
    "- вычислять по фолдам, то есть делить выборку на некоторое количество частей и подсчитывать значение признаков по всем фолдам кроме текущего (как делается в кросс-валидации)\n",
    "- внесение некоторого шума в посчитанные признаки (необходимо соблюсти баланс между избавление от переобучения и полезностью признаков).\n",
    "\n",
    "__Задание 3.__ Реализуйте корректное вычисление счётчиков двумя из трех вышеперчисленных способов, сравните. Снова обучите логистическую регрессию, оцените качество. Сделайте выводы.\n",
    "\n",
    "__(3 балла)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YX9gBIEJaRLW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59549\n",
      "Precision: 0.6020527423022164\n",
      "Recall: 0.5660060731980182\n",
      "F1 Score: 0.5834732018740668\n",
      "Time taken: 11.993443012237549 s\n",
      "CPU times: total: 12.7 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "# по объектам расположенным выше в датасете\n",
    "X = data.copy()  # создаем копию данных и целевой переменной\n",
    "y = target.copy()  # создаем копию целевой переменной\n",
    "for feature in categorical_features:  # проходим по всем категориальным признакам\n",
    "    ind = 0  # начальный индекс\n",
    "    ans = []  # список для хранения ответов\n",
    "    mp = {}  # словарь для хранения сумм и количеств значений признака\n",
    "    for j in X[feature].tolist():  # проходим по значениям признака\n",
    "        mp[j] = mp.get(j, [0, 0])  # добавляем значение в словарь, если его там нет\n",
    "        mp[j][0] += y[ind]  # добавляем к сумме значение целевой переменной\n",
    "        mp[j][1] += 1  # увеличиваем счетчик количества появлений значения\n",
    "        ans.append(mp[j][0] / mp[j][1])  # вычисляем среднее значение для значения признака\n",
    "        ind += 1  # увеличиваем индекс\n",
    "    X[feature] = pd.Series(ans, index=X[feature].index)  # заменяем значения признака на средние\n",
    "\n",
    "%time get(*train_test_split(normalize(X), y, test_size=0.5, random_state=73))  # запускаем функцию get с аргументами, полученными из train_test_split и normalize, и замеряем время выполнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# по фолдам\n",
    "X = data.copy()  # создаем копию данных и целевой переменной\n",
    "y = target.copy()  # создаем копию целевой переменной\n",
    "B = int(len(X) ** 0.5)  # определяем количество фолдов\n",
    "\n",
    "for feature in categorical_features:  # проходим по всем категориальным признакам\n",
    "    lst = X[feature].to_list()  # преобразуем значения признака в список\n",
    "    cnts = [{} for j in range(len(X) // B + 2)]  # список словарей для подсчета количества значений в каждом фолде\n",
    "    sums = [{} for j in range(len(X) // B + 2)]  # список словарей для подсчета суммы целевой переменной в каждом фолде\n",
    "\n",
    "    for i in range(len(X)):  # проходим по всем объектам\n",
    "        if len(cnts[i // B]) == 0:  # если текущий фолд пустой\n",
    "            cnts[i // B + 1] = cnts[i // B].copy()  # копируем предыдущий фолд\n",
    "            sums[i // B + 1] = sums[i // B].copy()  # копируем предыдущий фолд\n",
    "        cnts[i // B + 1][lst[i]] = cnts[i // B + 1].get(lst[i], 0) + 1  # увеличиваем счетчик для текущего значения\n",
    "        sums[i // B + 1][lst[i]] = sums[i // B + 1].get(lst[i], 0) + y[i]  # добавляем значение целевой переменной для текущего значения\n",
    "\n",
    "    ind = 0  # начальный индекс\n",
    "    ans = []  # список для хранения ответов\n",
    "    for j in X[feature].tolist():  # проходим по значениям признака\n",
    "        sum = sums[ind // B].get(j, 0) + sums[-1].get(j, 0) - sums[ind // B + 1].get(j, 0)  # сумма целевой переменной для текущего значения\n",
    "        cnt = cnts[ind // B].get(j, 0) + cnts[-1].get(j, 0) - cnts[ind // B + 1].get(j, 0)  # количество значений для текущего значения\n",
    "        ans.append(0 if cnt == 0 else sum / cnt)  # вычисляем среднее значение для значения признака\n",
    "        ind += 1  # увеличиваем индекс\n",
    "\n",
    "    X[feature] = pd.Series(ans, index=X[feature].index)  # заменяем значения признака на средние\n",
    "\n",
    "%time get(*train_test_split(normalize(X), y, test_size=0.5, random_state=73))  # запускаем функцию get с аргументами, полученными из train_test_split и normalize, и замеряем время выполнения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIXbvzlWaRLX"
   },
   "source": [
    "__Вывод:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbde96fnaRLV"
   },
   "source": [
    "## Часть 2. Метод опорных векторов и калибровка вероятностней"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvqNBbT4aRLV"
   },
   "source": [
    "__Задание 1.__ Обучение и применение метода опорных векторов.\n",
    "\n",
    "__(1 балл)__\n",
    "\n",
    "Обучите метод опорных векторов (воспользуйтесь готовой реализацией LinearSVC из sklearn). Используйте уже загруженные и обработанные в предыдущей части данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "G4eGtEwzaRLV"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC  # импортируем класс LinearSVC из модуля sklearn.svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalize(X), y, test_size=0.5, random_state=73)  # разделяем данные на обучающий и тестовый наборы\n",
    "\n",
    "svc = LinearSVC(random_state=73)  # создаем модель LinearSVC с заданным random_state\n",
    "\n",
    "svc.fit(X_train, y_train)  # обучаем модель на обучающих данных\n",
    "\n",
    "y_pred = svc.predict(X_test)  # делаем прогноз на тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reSZTM1daRLV"
   },
   "source": [
    "На той же тестовой части посчитайте все те же метрики. Что вы можете сказать о полученных результатах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0jW0b67TaRLV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.60508\n",
      "Precision: 0.6122444642781248\n",
      "Recall: 0.575575355601726\n",
      "F1 Score: 0.5933439051012211\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)  # вычисляем точность прогноза\n",
    "precision = precision_score(y_test, y_pred)  # вычисляем точность\n",
    "recall = recall_score(y_test, y_pred)  # вычисляем полноту\n",
    "f1 = f1_score(y_test, y_pred)  # вычисляем F1-меру\n",
    "\n",
    "print(f\"Accuracy:\", accuracy)  # выводим точность\n",
    "print(f\"Precision:\", precision)  # выводим точность\n",
    "print(f\"Recall:\", recall)  # выводим полноту\n",
    "print(f\"F1 Score:\", f1)  # выводим F1-меру\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECd18zToaRLW"
   },
   "source": [
    "__Вывод:__"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
