{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ __(5 pts)__ Pick one of environments: MountainCar-v0 or LunarLander-v2.\n",
    "  * For MountainCar, get average reward of __at least -150__\n",
    "  * For LunarLander, get average reward of __at least +50__\n",
    "\n",
    "See the tips section below, it's kinda important.\n",
    "__Note:__ If your agent is below the target score, you'll still get most of the points depending on the result, so don't be afraid to submit it.\n",
    "  \n",
    "  \n",
    "* __2.2__ __(bonus: 5 pts each)__ Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params), show graphs for different params\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. \n",
    "* 20-neuron network is probably not enough, feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "env = gym.make(\"MountainCar-v0\").env #choose 1 env - for example: gym.make(\"MountainCar-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=10000):\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        try:\n",
    "            probs = agent.predict_proba([s.tolist()])[0]\n",
    "        except Exception:\n",
    "            probs = [1. / n_actions for i in range(n_actions)]\n",
    "        a = np.random.choice(np.arange(n_actions), p=probs)\n",
    "        new_s,r,done,info = env.step(a)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def show_progress(rewards_batch,log,percentile=40, reward_range=[-5000,+5000]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch,percentile)\n",
    "    log.append([mean_reward,threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward,threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0],label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1],label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.subplot(1,2,2)\n",
    "    reward_range=[-1010,+10]\n",
    "    plt.hist(rewards_batch,range=reward_range);\n",
    "    plt.vlines([np.percentile(rewards_batch,percentile)],[0],[100],label=\"percentile\",color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "    \n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\" \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile) #recalc threshold. hint : np.percentile\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            elite_states.append(states_batch[i])\n",
    "            elite_actions.append(actions_batch[i])\n",
    "    if(len(np.array(elite_states[0]).shape) == 1):\n",
    "        elite_states = np.hstack(elite_states)\n",
    "    else:\n",
    "        elite_states = np.vstack(elite_states)\n",
    "    elite_actions = np.hstack(elite_actions)\n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),#(experiment with layers size),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "n_sessions = 100\n",
    "percentile = 40\n",
    "log = []\n",
    "startTime = time.time()\n",
    "for i in range(10000):\n",
    "    sessions = [generate_session() for _ in range(n_sessions)]\n",
    "    states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
    "    elite_states, elite_actions = select_elites(states_batch,actions_batch,rewards_batch,percentile=percentile)\n",
    "    agent.fit(elite_states, elite_actions)\n",
    "    show_progress(rewards_batch,log,percentile)\n",
    "    if np.array(rewards_batch).mean() >= -150:\n",
    "        print(\"Win!\")\n",
    "        break\n",
    "print(\"Time:\", time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20, 20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True,\n",
    "                      max_iter=1\n",
    "                     )\n",
    "n_sessions = 100\n",
    "percentile = 40\n",
    "log = []\n",
    "startTime = time.time()\n",
    "def generate_session(_):\n",
    "    return generate_session()\n",
    "with Pool() as pool:\n",
    "    sessions = pool.map(generate_session, range(n_sessions))\n",
    "states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile=percentile)\n",
    "agent.fit(elite_states, elite_actions)\n",
    "show_progress(rewards_batch, log, percentile)\n",
    "if np.array(rewards_batch).mean() >= -150:\n",
    "    print(\"Win!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
